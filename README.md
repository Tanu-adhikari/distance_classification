***Plots & Results:***
![Screenshot 2025-02-26 200049](https://github.com/user-attachments/assets/436a3856-fe80-47a2-ac94-69d3198c5089)
![Screenshot 2025-02-26 200102](https://github.com/user-attachments/assets/31362b20-bbd0-47cf-8f4b-2840042d55e9)
![Screenshot 2025-02-26 200436](https://github.com/user-attachments/assets/d2660697-6264-4ef0-921d-70c3972f49b9)
![shashi_tharoor_face](https://github.com/user-attachments/assets/73cb7155-2425-4404-a989-e46a1674cd7c)

1. What are the common distance metrics used in distance-based classification algorithms?
Ans- The common distance metrics used are:
        1. Euclidean Distance
        2. Mahalanobis Distance
        3. Manhattan Distance
        4. Chebyshev Distance

3. What are some real-world applications of distance-based classification algorithms? 
Ans- Distance-based algorithms are mainly used for image recognition, customer segmentation, medical diagnosis, recommendation systems, anomaly detection.

4. Explain various distance metrics.
Ans- 1. Euclidean Distance- Represents the shortest distance between two vectors.
     2. Mahalanobis Distance- Represents the distance between a point P and a distribution D.
     3. Manhattan Distance-  Distance between two points measured along axes at right angles.
     4. Chebyshev Distance-  It is defined on a vector space where the distance between two vectors is the greatest of their differences along any coordinate dimension.

5. What is the role of cross validation in model performance? 
Ans- Cross-validation is a technique for evaluating ML models by training several ML models on subsets of the available input data and evaluating them on the complementary subset of the data. We can use cross-validation to detect overfitting, ie, failing to generalize a pattern.

6. Explain variance and bias in terms of KNN?
Ans- The bias is an error from erroneous assumptions in the learning algorithm. High bias can cause an algorithm to miss the relevant relations between features and target outputs. The variance is an error from sensitivity to small fluctuations in the training set. High variance can cause an algorithm to model the random noise in the training data, rather than the intended outputs. In other words, model with high variance pays a lot of attention to training data and does not generalize on the data which it hasn’t seen before.
